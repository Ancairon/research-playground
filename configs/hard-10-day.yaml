# LSTM-Attention Best Configuration
# Generated: 2025-11-26T12:25:42.958035
# MAPE: 48.28%
# RMSE: 28.80
# Train Time: 2.8s

model: lstm-attention
single-shot: true
train-window: 7080
horizon: 2880
lookback: 4200
hidden-size: 32
num-layers: 3
dropout: 0.2
learning-rate: 0.0005
epochs: 150
batch-size: 256
use-differencing: false
scaler-type: robust
server-port: 5000

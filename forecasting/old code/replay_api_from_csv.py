"""
replay_api_from_csv.py

Read a CSV generated by `pattern_api_benchmark.py` (which contains timestamps and
values) and sequentially call the API for each row, capturing latency/errors and
writing an output CSV with the API responses and an aggregate summary.

Usage:
  python3 forecasting/replay_api_from_csv.py --in pattern_api_metrics.csv --rate 1 --out replayed_api.csv

"""
from __future__ import annotations

import argparse
import csv
import os
import time
from typing import List

from pattern_api_benchmark import DummyAPI
import json
import urllib.request
import urllib.parse


def _fetch_netdata_metric(base_url: str, chart: str, dimension: str | None = None, timeout: float = 5.0):
    """Fetch the latest value for `chart` and optional `dimension` from Netdata.

    Returns (value, latency_s) or raises an exception on failure.
    """
    # request latest 1 point
    params = {'chart': chart, 'points': 1}
    url = f"{base_url.rstrip('/')}/api/v1/data?{urllib.parse.urlencode(params)}"
    t0 = time.time()
    with urllib.request.urlopen(url, timeout=timeout) as resp:
        body = resp.read()
    latency = time.time() - t0
    data = json.loads(body)
    # `data` may contain 'data' key with list of rows, and 'labels' describing dimensions
    if 'data' not in data or not data['data']:
        raise RuntimeError('no data returned from netdata')
    last_row = data['data'][-1]
    # last_row may be [timestamp, dim1, dim2, ...]
    labels = data.get('labels') or []
    # try to find index matching the dimension label
    idx = None
    if dimension and labels:
        for i, lbl in enumerate(labels):
            if lbl == dimension or (isinstance(lbl, (list, tuple)) and dimension in lbl):
                idx = i
                break
    # fallback: pick first numeric after timestamp
    if idx is None:
        # assume timestamp at position 0
        if len(last_row) >= 2:
            return float(last_row[1]), latency
        raise RuntimeError('unable to extract metric value from netdata response')
    # if labels included timestamp as first label, adjust
    # if idx points to a label index, return last_row[idx]
    try:
        return float(last_row[idx]), latency
    except Exception as e:
        raise RuntimeError(f'parsing netdata value failed: {e}')


def replay(in_path: str, out_path: str, rate: float = 1.0, retries: int = 0, netdata_url: str | None = None, netdata_chart: str | None = None, netdata_dim: str | None = None):
    if not os.path.exists(in_path):
        raise SystemExit(f"input CSV not found: {in_path}")

    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    with open(in_path, 'r') as inf, open(out_path, 'w', newline='') as outf:
        reader = csv.DictReader(inf)
        fieldnames = reader.fieldnames + ['api_latency_s', 'api_error', 'prediction']
        writer = csv.DictWriter(outf, fieldnames=fieldnames)
        writer.writeheader()

        api = DummyAPI()

        start = time.time()
        sent = 0
        for row in reader:
            value = float(row.get('value', 0.0))
            attempt = 0
            success = False
            latency = 0.0
            pred = ''
            err = ''
            if netdata_url and netdata_chart:
                # Query Netdata API for the metric
                while attempt <= retries and not success:
                    try:
                        val, latency = _fetch_netdata_metric(netdata_url, netdata_chart, netdata_dim)
                        pred = val
                        success = True
                    except Exception as e:
                        attempt += 1
                        err = str(e)
                        if attempt > retries:
                            break
            else:
                # default: call the dummy API
                while attempt <= retries and not success:
                    t0 = time.time()
                    try:
                        res = api.predict(value)
                        latency = float(res.get('latency_s', 0.0))
                        pred = res.get('prediction')
                        success = True
                    except Exception as e:
                        attempt += 1
                        err = str(e)
                        if attempt > retries:
                            latency = time.time() - t0
                            break

            row['api_latency_s'] = latency
            row['api_error'] = '' if success else err or 'failed'
            row['prediction'] = pred
            writer.writerow(row)
            outf.flush()
            sent += 1
            # throttle to rate
            time.sleep(max(0.0, 1.0 / rate))

    print(f"Replayed {sent} rows to API, output written to {out_path}")


def main(argv: List[str] | None = None) -> int:
    p = argparse.ArgumentParser(description='Replay API from CSV')
    p.add_argument('--in', dest='in_path', required=True, help='Input CSV path')
    p.add_argument('--out', dest='out_path', required=True, help='Output CSV path')
    p.add_argument('--rate', type=float, default=1.0, help='Calls per second')
    p.add_argument('--retries', type=int, default=0, help='Retries per call on failure')
    p.add_argument('--netdata-url', dest='netdata_url', default=None, help='Base Netdata URL, e.g. http://localhost:19999')
    p.add_argument('--netdata-chart', dest='netdata_chart', default=None, help='Netdata chart id to query')
    p.add_argument('--netdata-dim', dest='netdata_dim', default=None, help='Netdata dimension label to extract')
    args = p.parse_args(argv)

    replay(args.in_path, args.out_path, rate=args.rate, retries=args.retries, netdata_url=args.netdata_url, netdata_chart=args.netdata_chart, netdata_dim=args.netdata_dim)
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
